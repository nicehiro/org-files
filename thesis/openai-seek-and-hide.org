* Introduction
监督学习无法完成复杂性很高的任务，所以人们现在在非监督学习领域探索比较多。然而，当前无方向探索方法在复杂的环境中表现很差，而且这种方法训练也和地球上生物的进化方式不同，地球上的生物是靠合作与竞争来进化的。根据合作和竞争的思想，有很多人完成了比较好的成果。

这篇文章的主要贡献是：
1. 明确的证据表明，多智能体 self-play 可以诱导智能体自发性学习
2. 这种自发性学习可以学到与人类行为相似的行为，例如使用工具

* Related Work
前人的许多多智能体使用工具的研究中，都明确激励了智能体和工具的交互，而在这篇文章的环境中，通过多智能体的竞争隐性的设定了这种交互。

* Hide and Seek
捉迷藏游戏。

* Policy Optimization
智能体包含两套网络，一个策略网络输出动作，一个 Critic 网络评测将来的 Return。使用 PPO 和 GAE 训练，使用集中训练，分布执行的方法。

所有智能体共享相同的策略网络参数（？为什么 Hider 和 Seeker 可以共享一套参数）。

* Words
- acquisition 获得物
- emergent 新兴的
- autocurricula 自发学习
- Intrinsic 本质的，固有的
