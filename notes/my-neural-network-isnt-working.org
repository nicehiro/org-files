#+TITLE: 我的神经网络训练的很垃圾！我该怎么办？
#+DATE: 2020-05-31
#+STARTUP: latexpreview
#+SETUPFILE: ~/.emacs.d/org-templates/level-1.org

本文翻译自 [[http://theorangeduck.com/page/neural-network-not-working][My Neural Network isn't working! What should I do?]]

在你研究深度学习下一个重大突破的时候，你遇到了非常不幸的事情：你的神经网络无法训练出好的效果，而且你还不知道该怎么去调节它。你跑去找你老板，但发现他们和你一样不知道该怎么调节，你陷入了困境。

幸好你看到了这篇文章，在这里我总结了我在实现神经网络和监督其他学生项目时的一些经验总结：

* 你忘记标准化输入数据了
** What?
在使用神经网络时，必须仔细考虑如何标准化数据。这个步骤不能缺失，如果你没有正确的进行标准化，你的网络几乎不能很好的运作。由于这一步骤非常重要，并且在深度学习社区广为人知，因此在论文中很好提及，因此几乎总是绊倒初学者。

** How?
一般来说，标准化是值——从数据中减去平均值，然后再除以标准差。通常只对每个输入和输出进行标准化，但是你可能希望对一组功能执行此操作。

** Why?
我们需要标准化数据的最主要原因是，神经网络的大多管道（参数设置）都是假设输入和输出数据均服从大约为 1 的标准偏差和大约为 0 的均值分布。这些假设出现在深度学习文献的任何地方，从权重初始化，到激活函数，再到训练网络的优化算法。

** And?
一个未被训练的神经网络输出的范围通常是从 -1 到 1 。如果你想让输出到其他范围，（比如说，RGB 图片的输入在 0 到 255），可能会遇到一些问题。当我们期望的输出是 255 时，但网络的输出只有 -1 到 1 ，着产生的误差是巨大的。这将会产生巨大的梯度，导致网络梯度爆炸。如果你运气特好，网络没有在一开始梯度爆炸，但训练的前几个阶段还是浪费，因为网络要学习的第一件事就是将输出值缩放并将其移动到大致所需的范围内。如果你可以标准化你的数据，那就不会遇到这些问题了。

通常，神经网络中特征的规模也将决定其重要性。如果输出中某个特征有很大的规模，那么与其他特征相比，这个特征可以造成更大的误差。同理，输入中规模较大的特征也将会主导网络并对下游的网络产生较大的影响。因此，使用许多神经网络库的自动归一化也是不够的，这些库中的函数盲目的减去均值，并按每个特征除以标准差。假设你有一个输入特征，范围在 0.0 到 0.001 —— 可能是个不重要的特征（你不想去扩张它），也可能这个特征和其他特征比起来不在一个量级上。谨慎使用范围很小的特征（即其标准差接近或者就是 0），如果对这些数据进行标准化，将会产生 NaN 的不稳定性。一定要考虑清楚每个特征代表着什么，标准化的过程是为了让所有输入特征的“单位”相等。这是我认为在学习深度学习过程中首先要搞清楚的一个问题。

* 你忘记检查输出结果了
** What?
或许你已经训练了一会网络了，而且可以看到误差在减少，这是否意味着你成功了呢？不，这只是你想多了。几乎可以确定，你的代码仍然存在问题。可能是数据预处理过程中，训练网络的过程中发生的错误，仅仅因为错误减少并不意味着你的网络正在学习任何有用的东西。

** How?
在训练的过程中检查数据训练的是否正确是非常重要的。通常，可以通过数据可视化的方法来检验数据。

** Why?
不像传统编程，机器学习极容易训练失败。传统的编程中，我们通过机器运行抛出异常来定位和解决问题。可惜，这个过程在机器学习的训练并不存在。因此在机器学习的过程中，我们需要时刻靠人眼盯着数据，才能知道训练的过程是否正确。

** And?
有很多方法可以检查你的网络是否正常运作。其中之一就是确定训练结果中发现的错误是什么。我的建议是从网络一开始就可视化所有内容，而不是仅当网络不能正常工作时才开始可视化

* 你忘记预处理你的数据了
** What?
大多数数据都是很难处理的，而且常常是我们知道的比较相似的数据，在数值表示上有很大的差别。以角色动画为例——如果我们使用角色关节的 3D 相对位置来表示数据，则在不同位置或面向不同方法执行相同的动作可能会得到不同的数据。我们需要做的是，使用不同方式表示数据，以便我们知道两个动作是相似的，并用相似的数值表示出来。

** How?
想想你网络的特征真正代表的是什么。是否有一些简单的变换可以确保相同意义的数据具有相同的数量级。是否有局部座标系来表示数据，从而使数据变得更自然——更好的色彩空间——不同的格式？

** Why?
神经网络进对它们输入的数据做一些基本假设——这些基本假设之一就是数据所在的空间是连续的——对于大多数空间，两个数据点之间的点至少在某种程序是着两个数据点的混合，并且数据点附近的两个数据点在某种意义上表示相似的事物。数据空间中的不连续性很大，或者代表相同事物的大量单独数据簇将使学习任务变得更加困难。

** And?
考虑数据预处理的另一种方法是尝试减少可能需要的数据变化的组合爆炸式增长。如果训练角色动画数据的神经网络必须在每个位置和方法上学习角色的相同运动，那么这将浪费网络的很多功能，并且会重复很多学习过程。

* 你忘记使用正则化了
** What?
正则化——通常是以 dropout，噪声或某种形式的随机过程注入网络中的方法，是训练现代神经网络的另一个重要方面。即使你认为自己的数据多得多，或者在某些情况下过你和无关精要，添加 dropout 和其他形式的噪声仍然是有帮助的。

** How?
规范化神经网络的最基本方法是在网络的每个线性层（卷积或全连接）之前添加 dropout 层。根据你认为过拟合的可能性或其他你认为影响的因素去调节原始数据的保留概率。如果你认为过拟合的可能性很小，可以考虑将保留概率设为很高的值，例如 0.99 。

** Why?
正则化不仅仅是用来解决过度拟合问题。通过在训练过程中加入一些随机过程，可以在某种程度上使你的损失曲线更加丝滑。这样可以加快训练速度，帮助处理数据中的噪声和异常值，并防止网络的权重过大。

** And?
数据丢失或其他类型的噪声也可以想 dropout 一样充当正则化，有时，当缺失组够多的数据时，dropout 层也可以不必添加。

* 你使用的 Batch Size 太大了
** What?
使用过大的 batch size 会对神经网络的训练产生负面的影响，因为它减小了梯度下降采样的随机性。

** How?
使用你可以忍受训练时间的最小 batch size 。最佳利用 GPU 并行性的批处理大小可能不是最佳的批处理大小，因为在某些时候，较大的批处理大小将需要训练网络更多的时间来达到相同的精度。不要害怕从很小的批量开始，比如 16， 8，甚至可以是 1。

** Why?
使用小的批处理大小，会产生更琐碎，更随机的权重更新过程。这将会带来两个好处：第一：可以跳出局部最优解；第二：它可能导致训练趋于稳定，这通常表示更好的泛化性能。

** And?
数据中的某些其他元素有时也可以像批处理大小一样有效的起作用。例如，以两倍于以前的分辨率处理图像，会产生与使用四倍批处理大小的图像相似的效果。为了对此有个直观的认识，可以考虑在 CNN 中，每个 filter 的权重更新将在输入图像以及应用在批次中的每个图像的像素上平均。将图像分辨率提高两倍的效果，与将批处理大小增加四倍的效果非常相似。总的来说，重要的是要考虑每次迭代，最终梯度更新的平均值是多少，并确保平衡所产生的不利影响尽可能多的使用 GPU 的潜在并行性。

* 你使用了错误的学习率
** What?
学习率的设置可能会对网络训练的难易程度产生巨大影响，如果你是新手，几乎可以确定，由于通用深度学习框架中使用了各种默认选项，你的设置大概率是不正确的。

** How?
关闭梯度裁剪。找到学习率可以设置的最高值，该值不会使错误在训练过程中爆炸。将学习率设置的比上诉值低一个数量级，这可能是最接近最佳学习率的值了。

** Why?
许多深度学习框架默认都开启梯度裁剪。此选项通过强制设置每个步骤可以改变权重的最大值来防止训练过程中权重爆炸。如果你的数据中包含许多异常值，这些异常值会产生较大的误差，从而导致较大的梯度和权重更新，那么这个选项就很有用。但同时，启动它也意味这很难手动找到最佳学习率。我发现，大多数深度学习的新手都将学习率设置的太高，以至于梯度裁剪都无法很好的解决。这会使整体训练行为变慢，并且改变学习率的效果无法预测。

** And?
如果你合理的清洗过你的数据，删除了大多数的异常值，并正确设置了学习率，那么你实际上就不需要进行梯度裁剪了。如果不开启梯度裁剪，你发现在训练过程中错误会偶尔发生爆炸，那么你可以打开梯度裁剪再尝试一下。但请记住，训练错误不断爆炸的原因几乎都是因为你的某些数据还有其他问题，梯度裁剪只是一个临时解决的方案。

* 你在输出层使用了错误的激活函数
** What?
在输出层使用激活函数意味着你的网络不能产生全范围的所需值。最常见的错误是在输出层使用 ReLU 函数，导致网络智能输出正值。

** How?
如果你在做回归相关的工作，那么输出层一般不需要任何激活函数，除非你对输出的值非常了解。

** Why?
再想想你的数据值实际代表什么，标准化之后它们的范围是什么。很有可能你想要的输出就是无界的正数或者负数，这种情况下你就不需要在最后一层使用激活函数了。如果你想要的输出是在某个范围内的，比如概率分布在 0 到 1 之间，可以使用特定的激活函数，比如 sigmoid 函数。

** And?
在最后一层使用激活函数有很多微妙之处。也许你知道你的系统会将神经网络最后的输出裁剪到 -1 到 1 之间。将此裁剪过程作为输出层的激活函数看起来好像有点用，因为这个函数将确保网络不会输出大于 1 或者小于 -1 的值。但是没有错误意味着这些值也不会存在梯度，这可能会导致网络无法训练。有人可能会使用 tanh 作为最后一层的输出，因为这个函数的输出范围在 -1 到 1 之间。但这个函数也可能会产生问题，因为 tanh 的 -1 或 1 附近的梯度会变的非常小，这可能会导致网络的权重增大，从而试图精确生成 -1 或 1。因此最好的选择通常是在最后一层不使用任何激活函数，不要做一些可能事与愿违的聪明事。

* 你的网络包含坏梯度
** What?
使用 ReLU 激活函数的网络通常会遭受由坏梯度引起的，被称为“失活神经元”的困扰。这会对网络的性能产生负面影响，甚至在某些情况下无法进行训练。

** How?
如果你发现训练的损失在随着训练进程的发展而变化了，那可能是由于使用 ReLU 激活函数导致神经元死亡造成的。你可以尝试换其他的激活函数进行训练，比如 leaky ReLUs 或者 ELUs。

** Why?
ReLU 激活函数的梯度是：正值为 1，负值为 0。这是因为输入的很小变化不会影响小于 0 的输入的输出。由于正值的梯度很大，因此这似乎不是一个立即出现的问题，但如果有多层堆叠，负权重可以将具有较大正值的梯度改为具有零梯度的负值。此时，无论给什么输入，其损失函数的梯度都是 0。这种情况我们称为网络失活了，其权重无法继续被更新了。

** And?
任何于计算权重导数相关的操作，比如 clipping，舍入，取最大最小值，都会产生不良梯度。如果这些符号出现在代码中，请务必小心，因为它们通常会导致无法预料的错误。

* 你的网络参数初始化有问题
** What?
如果你的网络参数初始化有问题，那你的网络很有可能不会被训练好。许多神经网络的框架都采用某种形式的分布来进行权重初始化，并将偏置值设为 0，但这并没有用，或者使用你自己设定的随机初始化也没有用。

** How?
不同的激活函数对应使用不同的初始化方法。你可以进行不同的尝试，最后选择在训练过程中表现最好的那种方法。

** Why?
或许你听说过，初始化网络权重需要使用“很小的随机的数值”，但初始化并没有这么简单。‘he’，‘lecun’，‘xavier’这些方法都是经过数学公式推导证明的，都解释了为什么那样初始化可以达到最佳的效果。而且，这些初始化方法都是大家在训练过程中共用的一套方法，如果你使用了其他初始化方法，那么复现其他研究人员的结果将变得非常困难。

* 你的神经网络太深了
** What?
神经网络层数越深越好吗？当我们已经取得了不错的训练效果，想要再更进一步时，可以通过增加网络层数尝试一下。但如果你的网络在 3，4，5 层时没有一点效果，那就算你加到 100 层也是没有用的。

** How?
首先从 3 到 8 层的浅层神经网络开始尝试，当你开始研究如何提高准确性时，可以开始尝试使用更深的网络。

** Why?
如果你的神经网络训练不好，一般都是其他问题，而不是网络深度的问题。

** And?
从小网络开始训练，也意味这你的网络训练会变得更快，迭代的也会更快。所有这些带来的收益都要比简单的堆叠更多的层数要多得多。

* 你的神经网络隐藏层神经元数量设置的有问题
** What?
在某些情况下，使用太多或者太少的神经元都会使你的网络很难训练。神经元太少，网络无法满足复杂任务的需求；神经元太多，会导致训练缓慢。

** How?
神经元数量的选取一般在 256 到 1024 之间。看看从事类似研究工作的研究人员使用的神经元数量并从中找取灵感。如果相关研究人员给出的数字与上面给出的数字大不相同，那必定是由特定原因造成的。

** Why?


** And?
实际上，与其他因素相比，隐藏单元的数量通常对神经网络的性能影响很小，而且在许多情况下，高估所需隐藏单元的数量除了降低训练速度之外，几乎没有负面影响。如果您仍然担心，只需尝试一大堆不同的数字并测量准确性，直到找到最合适的数字。
