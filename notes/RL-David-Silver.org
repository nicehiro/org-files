#+TITLE: 强化学习 -- David Silver Course
#+OPTIONS: toc:nil
#+DATE: 2019-06-06
#+SETUPFILE: ~/.emacs.d/org-templates/level-1.org

* Lession 1
** 强化学习和其他机器学习之间的区别是什么？
- 没有监督者，只有反馈信号
- 反馈会有延迟，不是立刻就能得到反馈
** Agent & Environment
#+BEGIN_SRC dot :file ../img/agent-environment.png :export result
digraph agent_environment {
forcelabels=true
splines=false
  "Agent"[shape="ellipse"]
  "Environment"[shape="ellipse"]
  "Agent":sw->"Environment":nw[dir=forward, xlabel=" Action "]
  "Environment":ne->"Agent":se[dir=backward, xlabel=" State, Reward "]
}
#+END_SRC

#+RESULTS:
[[file:../img/agent-environment.png]]

** History
\[ H_t = A_1, O_1, R_1, .... , A_t, O_t, R_t \]
#+BEGIN_SRC quote
H: History
A: Action
O: Observation
R: Reward(from environment)
#+END_SRC

推动事件发生的动力取决于History。
- 构建一个History到 Action的映射，Agent根据这个映射选择Action
- Environment返回Agent Observation 和 Rewards
** State
State是用来决定下一个Action的信息状态。
\[ S_t = f(H_t) \]

Environment State(\(S_t^e\))，对于agent来说，可能不是可见的，也可能会包含过多无用的信息。
Agent State(\(S_t^a\))，是把 \(S_t^e\) 经过agent抽象、整理之后的State，是RL用来分析的State。
*** Markov
如果一个State满足：
\[ P[S_{t+1} | S_t] = P[S_{t+1} | S_1, S_2, ... S_t] \]
那么这个状态就是Markov
简单来说，P的下一个阶段的state（\(P_{t+1}\)）只和当前state（\(P_t\)）有关，与之前的状态无关，那就称这个state为Markov。
** RL Agent的主要元素
- Policy 决策，agent的行为函数（agent采取什么action取决于这个函数）
- Value function 评价函数，用来判断当前状态的好坏，当前action的好坏
- Model agent对当前环境的表述
*** Policy
- 从state到 action 的映射 mapping
\[ a = \pi(s) \]
*** Value Function
- 对未来reward的预测函数
- 评价state的好坏
- 评价函数会计算未来一段时间的累积reward值的大小，而不只是当下。但是也不能考虑太远的未来，因为未来是无限长的，而且也没有必要去计算那么长的未来，所有会添加discount因子，即下述的\(\gamma\)
\[ v_{\pi}(s) = E{\pi}[R_t + \gamma R_{t+1} + \gamma ^2R{t+2} + ... | S_t = s] \]
** RL Agent的分类
*** 分类1
- Value based
- Policy based
- Actor Critic(Both)
*** 分类2
- Model Free
  + Policy and/or Value Function
  + No Model
- Model Based
* Lession 2
** State Transition Matrix 状态转移矩阵
\(P_{ss^{\prime}}\)表示State从 s 转移到 \(s^{\prime}\)的概率。
\[ P_{ss^{\prime}} = P[S_{t+1} = s^{\prime} | S_t = s] \]
** Markov Reward Process
Markov Reward Process 可以用元祖表示 (S, P, R, \(\gamma\))
- S 有限的State集合
- P 状态转移矩阵
- R Reward Function 反馈方程
- \(\gamma\) discount因子
*** Return
Return（Environment 给 Agent）\(G_t\)是从 t 阶段之后的总的discounted reward之和。
\[ G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty}\gamma ^kR_{t+k+1} \]
*** Why discount
- 数学的可行性、容易性
- Markov Process 是无限的，不可能无限的考虑到之后所有的reward
- 未来的不确定性，无法完美的表征未来
- 个例
#+BEGIN_SRC quote
额，我上节课作出的回答不够完善，只想出了第二个原因
#+END_SRC
*** Value Function
Value Function 会给出当前状态下的预计return（\(G_t\)）
\[ v(s) = E[G_t | S_t = s] \]
*** Bellman 等式
上诉的value function可以拆分开（动态规划）：
- 立即反馈 \(R_{t+1}\)
- discounted后续的 \(\gamma v(S_{t+1})\)
\begin{align}
 v(s) &= E[ G_t | S_t = s ]\\
&= E[ R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + ... | S_t = s ]\\
&= E[ R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + ...) | S_t = s]\\
&= E[ R_{t+1} + \gamma G_{t+1} | S_t = s ]
\end{align}
** Markov Decision Process
Markove Decision Process 可以用元祖表示（S, A, P, R, \(\gamma\)）
和上诉的MRP相比多了：
- A：action的集合
MRP到某个state是通过概率的方式计算，而MDP加入了action，是agent主动选择某个action进入一个state
*** Policies
Agent的行为完全取决于Policy。
\[ \pi(a|s) = P[A_t = a | S_t = s] \]
*** Value Function
\(\pi\) ：policy
- state-value function
  \[ v_{\pi}(s) = E_{\pi}[G_t | S_t = s] \]
- actor-value function
  \[ q_{\pi}(s, a) = E_{\pi}[G_t | S_t = s, A_t = a] \]

上诉两个value function传入的State有不同的含义。当我们现在在\(S=s_1\)，考虑下一步改执行什么action以及得到什么state时，state-value function传入的state是下一步的状态\(s_2\)；actor-value function传入的state是当前的状态\(s_1\)
*** Bellman Expectation Equation
\[ v_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s] \]

\[ q_{\pi}(s, a) = E_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a] \]

核心和之前相同，当前状态的value，由下一个状态的即时reward和discounted下个状态的value来决定。
*** Optimal Policy
\[ \pi > \pi^{\prime} \text{ if } v_{\pi}(s) > v_{\pi^{\prime}}(s) \]
* Lession 3
** 动态规划
- 可以被分解成子问题
- 子问题都是相同且重复的
** 为啥使用动态规划的方法解决RL？
- Bellman Equation给出了循环迭代的等式
- Value function可以被存储，之后使用到某个节点的value时可以重复使用，并且被不停更新

* 心得
- 不要被符号所局限，也不要完全脱离数学符号。
- 听不懂的先跳过，因为你一直重复听也是听不懂的
