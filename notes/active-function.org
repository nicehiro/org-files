#+TITLE: 激活函数
#+DATE: 2019-11-25
#+TOC: nil
#+SETUPFILE: ~/.emacs.d/org-templates/level-1.org

* 背景
如果没有激活函数（Active Function），神经网络可以看作特征向量的线性组合来拟合函数得到不同输入对应的输出。可以想象这样的拟合只能是线性的，应对稍微复杂点的问题效果就会很差。激活函数的作用就是“掰弯”拟合函数。

* 神经网络的参数更新
神经网络使用反向传播来更新参数，具体的更新方法参见[[http://nicehiro.me/notes/back-propagation.html][反向传播算法]]。参数的更新过程中会涉及到激活函数的求导，因此选一个好的激活函数会影响到参数更新的效果。

* 激活函数
** Sigmoid
Sigmoid 又叫做 Logistic 激活函数，它将实数值压缩到0到1的区间内，因此经常在预测概率的输出层中使用。其公式为：
\begin{equation}
\sigma(x) = \frac{1}{1 + \exp^{-x}}
\end{equation}

函数图像为：

#+BEGIN_CENTER
#+ATTR_HTML: :width 80%
[[file:../img/active-func-2.png]]
#+END_CENTER

可以看出：
- Sigmoid 函数只有在中间[-6, +6]区间内有些梯度，此范围之外梯度几乎为0。即，反向传播时，输出接近 0 或者 1 的神经元梯度几乎为0。这些神经元叫做饱和神经元，它们的权重几乎不会更新。另外，此函数的梯度最大为 0.25，反向传播过程中梯度的更新会随着相乘越来越小，因此神经网络使用这个激活函数很难更新参数。
- 期望不为0
- 指数函数的求导相对于其他激活函数而言计算成本高

** Tanh
Tanh 激活函数又叫做双曲正切激活函数，其公式为：
\begin{equation}
tanh(x) = 2 \cdot sigmoid(2x) - 1
\end{equation}

函数图像为：

#+BEGIN_CENTER
#+ATTR_HTML: :width 80%
[[file:../img/active-func-1.png]]
#+END_CENTER

分析：
- 也存在梯度消失的问题，但其最大导数为1，相比 Sigmoid 来说要好一些
- 期望为0
- 计算成本高

** ReLU
ReLU 激活函数又叫做修正线性单元函数，其函数为：
\begin{equation}
ReLU(x) = max(0, x)
\end{equation}

其图像为：
#+BEGIN_CENTER
#+ATTR_HTML: :width 80%
[[file:../img/active-func-3.png]]
#+END_CENTER

分析：
- 取值为[0, inf]，可以起到放大结果的作用
- 正数的梯度为1，但负数的梯度为0，因此也会出现梯度消失的问题
- 期望不为0
- 梯度计算成本低

** Leaky ReLU
Leaky ReLU 激活函数的公式为：
\begin{equation}
LReLU(x) = max(ax, x), 0 \leq a \leq 1
\end{equation}

其图像为：
#+BEGIN_CENTER
#+ATTR_HTML: :width 80%
[[file:../img/active-func-4.png]]
#+END_CENTER

分析：
- 取值为[-inf, inf]
- 负数梯度不为1，小于1，虽然在一定程度上缓解了 ReLU 出现的梯度消失的问题，但仍然还是会出现这个问题
- 梯度计算成本低

** Parametric ReLU
Parametric ReLU 又叫做超参数 ReLU，其公式为：
\begin{equation}
PReLU(x) = max(\alpha x, x)
\end{equation}

其参数\(\alpha\)也是通过学习来确定的。其思想是，既然不能人为确定 ReLU 左侧的梯度值，不如再设一个参数也通过学习的方法来确定。

* Ref
- [[https://www.jiqizhixin.com/articles/2017-11-02-26][一文概览深度学习中的激活函数]]
- [[https://www.csuldw.com/2019/05/26/2019-05-26-activation-function/][神经网络之激活函数]]
